{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e026f7ee-e5a5-4053-92f7-4fe5bb84c4f5",
   "metadata": {},
   "source": [
    "# Development of RoBERTa Model\n",
    "\n",
    "## Supervised Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9aa161f0-78f5-4dcc-b834-0efa005f2811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import ast\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report, precision_score, recall_score\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7307853a-9496-4dea-8a72-48b3c9f8509a",
   "metadata": {},
   "source": [
    "### Import Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df4db691-b7be-449d-b655-5b9420888dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = 'combined_500ms_whisper_diarization_stable_ts_aligned'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a8aa5ea-1696-4e14-9e11-af360ed8bb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to store the data\n",
    "aligned_dict = {}\n",
    "\n",
    "# Iterate through files in the directory\n",
    "for file_name in os.listdir(dir_path):\n",
    "    # Check if the file is a CSV\n",
    "    if file_name.endswith('.csv'):\n",
    "        # Create the full file path\n",
    "        file_path = os.path.join(dir_path, file_name)\n",
    "        \n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Use the file name without the extension as the dictionary key\n",
    "        key = os.path.splitext(file_name)[0]\n",
    "        \n",
    "        # Store the DataFrame in the dictionary\n",
    "        aligned_dict[key] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dce81277-ce51-4132-959c-439e2ad7a5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(aligned_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fe1b5e0-b99e-4209-b666-e487e8e8f20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group keys by their first three digits\n",
    "grouped_keys = {}\n",
    "for key in keys:\n",
    "    prefix = key[:3]\n",
    "    if prefix not in grouped_keys:\n",
    "        grouped_keys[prefix] = []\n",
    "    grouped_keys[prefix].append(key)\n",
    "\n",
    "# Get list of prefixes\n",
    "prefixes = list(grouped_keys.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e531792-ac10-4065-bc81-f0e05bcb49a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prefixes =  ['051', '134', '251', '027', '129', '038', '108', '107', '252']\n",
    "all_train_prefixes = list(set(prefixes) - set(test_prefixes))\n",
    "train_prefixes, val_prefixes = train_test_split(all_train_prefixes, test_size=0.10, random_state=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1bc9eb8-01ad-4645-9e59-9e552f18b47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_keys = [key for key in keys if key[:3] in train_prefixes]\n",
    "val_keys = [key for key in keys if key[:3] in val_prefixes]\n",
    "test_keys = [key for key in keys if key[:3] in test_prefixes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad5cb6a3-9283-4106-9921-950547d1acf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dfs = [aligned_dict[key][['duration',\"speaker\", \"label\", \"transcript\"]] for key in train_keys]\n",
    "val_dfs = [aligned_dict[key][['duration',\"speaker\", \"label\",  \"transcript\"]] for key in val_keys]\n",
    "test_dfs = [aligned_dict[key][['duration',\"speaker\", \"label\", \"transcript\"]] for key in test_keys]\n",
    "\n",
    "train_df = pd.concat(train_dfs, ignore_index=True)\n",
    "val_df = pd.concat(val_dfs, ignore_index=True)\n",
    "test_df = pd.concat(test_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942c40d2-5471-488e-8afd-f0f5cf6d7d4f",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac1ae69d-c2c4-487c-8ccc-bd5974eaffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[train_df[\"speaker\"] != \"Student\"]\n",
    "val_df = val_df[val_df[\"speaker\"] != \"Student\"]\n",
    "test_df = test_df[test_df[\"speaker\"] != \"Student\"]\n",
    "\n",
    "train_df = train_df[train_df[\"label\"] != \"UNI\"]\n",
    "val_df = val_df[val_df[\"label\"] != \"UNI\"]\n",
    "test_df = test_df[test_df[\"label\"] != \"UNI\"]\n",
    "\n",
    "train_df = train_df[train_df[\"duration\"] > 0.5]\n",
    "val_df = val_df[val_df[\"duration\"] > 0.5]\n",
    "test_df = test_df[test_df[\"duration\"] >0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90a5644c-9659-48e6-b358-23db2beba06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['label'] = train_df['label'].apply(lambda x: ' '.join([label for label in x.split() if label not in ['UNI', 'SIL']]))\n",
    "val_df['label'] = val_df['label'].apply(lambda x: ' '.join([label for label in x.split() if label not in ['UNI', 'SIL']]))\n",
    "test_df['label'] = test_df['label'].apply(lambda x: ' '.join([label for label in x.split() if label not in ['UNI', 'SIL']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89316fcf-3514-48fe-9828-cf5cabe07276",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[~train_df[\"transcript\"].isnull()]\n",
    "val_df = val_df[~val_df[\"transcript\"].isnull()]\n",
    "test_df = test_df[~test_df[\"transcript\"].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ea8e6428-a2a4-402a-b99d-c36d870d0f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.FloatTensor(self.labels[idx])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6fdb32ce-ea43-459a-96fc-4d3eab20ce79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "97544379-a0a3-498f-aafd-748bd637bc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to multi-hot encoding\n",
    "mlb = MultiLabelBinarizer()\n",
    "# Fit on training data only to prevent data leakage\n",
    "train_labels = train_df['label'].str.split()\n",
    "mlb.fit(train_labels)\n",
    "\n",
    "# Transform all datasets\n",
    "train_encoded_labels = mlb.transform(train_labels)\n",
    "val_encoded_labels = mlb.transform(val_df['label'].str.split())\n",
    "test_encoded_labels = mlb.transform(test_df['label'].str.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0c43e93-7dad-4cc0-8bd2-ff02a60f5c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "51e796a8-0c67-4128-9914-3268f6740965",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "train_dataset = TextClassificationDataset(\n",
    "    train_df['transcript'].values, \n",
    "    train_encoded_labels,\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "val_dataset = TextClassificationDataset(\n",
    "    val_df['transcript'].values, \n",
    "    val_encoded_labels,\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "test_dataset = TextClassificationDataset(\n",
    "    test_df['transcript'].values, \n",
    "    test_encoded_labels,\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Initialize model\n",
    "model = RobertaForSequenceClassification.from_pretrained(\n",
    "    'roberta-base',\n",
    "    num_labels=len(mlb.classes_),\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aee85bf-91c3-4c9e-b89f-0f6c7f2a7ee4",
   "metadata": {},
   "source": [
    "## Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b8ca531b-caa7-4c98-b42f-94d32f18071f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "42e996ac-3e56-4f9e-b0aa-e36a9596a9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            loss = criterion(outputs.logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            predictions = torch.sigmoid(outputs.logits)\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return total_loss / len(dataloader), np.array(all_predictions), np.array(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712c52d4-6c50-432d-8920-963f0b469414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping parameters\n",
    "patience = 3  # Number of epochs to wait before early stopping\n",
    "min_delta = 0.001  # Minimum change in validation loss to qualify as an improvement\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "early_stop = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0f0b90-adf8-48a7-84e3-e8f7507ff135",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "832fbfe5-824e-41b4-94a7-55833695d393",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "Training Loss: 0.3369\n",
      "Validation Loss: 0.2807\n",
      "Validation Macro F1: 0.2585\n",
      "Validation Micro F1: 0.6969\n",
      "Saved best model!\n",
      "--------------------------------------------------\n",
      "Epoch 2:\n",
      "Training Loss: 0.2788\n",
      "Validation Loss: 0.2597\n",
      "Validation Macro F1: 0.3051\n",
      "Validation Micro F1: 0.7254\n",
      "Saved best model!\n",
      "--------------------------------------------------\n",
      "Epoch 3:\n",
      "Training Loss: 0.2469\n",
      "Validation Loss: 0.2649\n",
      "Validation Macro F1: 0.3399\n",
      "Validation Micro F1: 0.7380\n",
      "Validation loss didn't improve. Patience: 1/3\n",
      "--------------------------------------------------\n",
      "Epoch 4:\n",
      "Training Loss: 0.2197\n",
      "Validation Loss: 0.2588\n",
      "Validation Macro F1: 0.5225\n",
      "Validation Micro F1: 0.7530\n",
      "Validation loss didn't improve. Patience: 2/3\n",
      "--------------------------------------------------\n",
      "Epoch 5:\n",
      "Training Loss: 0.1924\n",
      "Validation Loss: 0.2537\n",
      "Validation Macro F1: 0.5069\n",
      "Validation Micro F1: 0.7572\n",
      "Saved best model!\n",
      "--------------------------------------------------\n",
      "Epoch 6:\n",
      "Training Loss: 0.1683\n",
      "Validation Loss: 0.2780\n",
      "Validation Macro F1: 0.4779\n",
      "Validation Micro F1: 0.7481\n",
      "Validation loss didn't improve. Patience: 1/3\n",
      "--------------------------------------------------\n",
      "Epoch 7:\n",
      "Training Loss: 0.1485\n",
      "Validation Loss: 0.2702\n",
      "Validation Macro F1: 0.5215\n",
      "Validation Micro F1: 0.7466\n",
      "Validation loss didn't improve. Patience: 2/3\n",
      "--------------------------------------------------\n",
      "Epoch 8:\n",
      "Training Loss: 0.1310\n",
      "Validation Loss: 0.2887\n",
      "Validation Macro F1: 0.5218\n",
      "Validation Micro F1: 0.7410\n",
      "Validation loss didn't improve. Patience: 3/3\n",
      "Early stopping triggered after 8 epochs!\n",
      "\n",
      "Training Summary:\n",
      "Best validation loss: 0.2537\n",
      "Training stopped after 8 epochs\n",
      "Reason: Early stopping\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_predictions, val_true_labels = evaluate(model, val_loader, criterion, device)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}:')\n",
    "    print(f'Training Loss: {train_loss:.4f}')\n",
    "    print(f'Validation Loss: {val_loss:.4f}')\n",
    "    \n",
    "    # Calculate and print metrics\n",
    "    val_pred_binary = (val_predictions > 0.5).astype(int)\n",
    "    \n",
    "    macro_f1 = f1_score(val_true_labels, val_pred_binary, average='macro')\n",
    "    micro_f1 = f1_score(val_true_labels, val_pred_binary, average='micro')\n",
    "    \n",
    "    print(f'Validation Macro F1: {macro_f1:.4f}')\n",
    "    print(f'Validation Micro F1: {micro_f1:.4f}')\n",
    "    \n",
    "    # Check if validation loss improved\n",
    "    if val_loss < best_val_loss - min_delta:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model_whisper_diarization.pt')\n",
    "        print(\"Saved best model!\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"Validation loss didn't improve. Patience: {patience_counter}/{patience}\")\n",
    "    \n",
    "    # Check early stopping condition\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping triggered after {epoch + 1} epochs!\")\n",
    "        early_stop = True\n",
    "        break\n",
    "        \n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Print training summary\n",
    "print(\"\\nTraining Summary:\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"Training stopped after {epoch + 1} epochs\")\n",
    "if early_stop:\n",
    "    print(\"Reason: Early stopping\")\n",
    "else:\n",
    "    print(\"Reason: Completed all epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9469012d-bce6-427b-90ca-86ea50654161",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "db47d0e3-b1ad-4287-9414-78770ca4893b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test Results:\n",
      "Test Loss: 0.2828\n",
      "Test Macro F1: 0.4849\n",
      "Test Micro F1: 0.7038\n",
      "BSP: F1 = 0.5799\n",
      "GPRS: F1 = 0.3463\n",
      "IST: F1 = 0.7608\n",
      "NEU: F1 = 0.1619\n",
      "RED: F1 = 0.2254\n",
      "REP: F1 = 0.3027\n",
      "ST: F1 = 0.4834\n",
      "SV: F1 = 0.7946\n",
      "aAFF: F1 = 0.6470\n",
      "aCORR: F1 = 0.3293\n",
      "aOTR: F1 = 0.7890\n",
      "sOTR: F1 = 0.3990\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation on test set\n",
    "model.load_state_dict(torch.load('best_model_whisper_diarization.pt'))\n",
    "test_loss, test_predictions, test_true_labels = evaluate(model, test_loader, criterion, device)\n",
    "test_pred_binary = (test_predictions > 0.5).astype(int)\n",
    "\n",
    "# Calculate final test metrics\n",
    "test_macro_f1 = f1_score(test_true_labels, test_pred_binary, average='macro')\n",
    "test_micro_f1 = f1_score(test_true_labels, test_pred_binary, average='micro')\n",
    "print(\"\\nFinal Test Results:\")\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'Test Macro F1: {test_macro_f1:.4f}')\n",
    "print(f'Test Micro F1: {test_micro_f1:.4f}')\n",
    "\n",
    "# Print label-wise performance\n",
    "for i, label in enumerate(mlb.classes_):\n",
    "    label_f1 = f1_score(test_true_labels[:, i], test_pred_binary[:, i])\n",
    "    print(f'{label}: F1 = {label_f1:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02872ec5-ed27-4c12-a6ff-7ddb95078e58",
   "metadata": {},
   "source": [
    "## Broader Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8d606668-9146-4a22-b9b1-6d319d357ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Broader concept frequencies in training set:\n",
      "               Broader Concept  Count  Percentage\n",
      "                  Teacher_Talk   3586   68.252760\n",
      "                         Other   3551   67.586601\n",
      "        Opportunity_to_Respond   2910   55.386372\n",
      "             Academic_Feedback    880   16.749144\n",
      "                        Praise    533   10.144652\n",
      "Corrective_Behavioral_Feedback    368    7.004187\n",
      "\n",
      "Distribution of number of broader concepts per instance:\n",
      "0 concepts: 2 instances (0.04%)\n",
      "1 concepts: 1053 instances (20.04%)\n",
      "2 concepts: 2310 instances (43.97%)\n",
      "3 concepts: 1460 instances (27.79%)\n",
      "4 concepts: 374 instances (7.12%)\n",
      "5 concepts: 51 instances (0.97%)\n",
      "6 concepts: 4 instances (0.08%)\n",
      "\n",
      "Sample mappings:\n",
      "Original: NEU -> Broader: Other\n",
      "Original: BSP ST aOTR SV -> Broader: Other Praise Opportunity_to_Respond Teacher_Talk\n",
      "Original: SV IST SV aAFF IST ST SV -> Broader: Academic_Feedback Other Teacher_Talk\n",
      "Original: NEU -> Broader: Other\n",
      "Original: NEU ST SV -> Broader: Other Teacher_Talk\n",
      "Original: SV aOTR SV -> Broader: Other Opportunity_to_Respond\n",
      "Original: aAFF SV -> Broader: Academic_Feedback Other\n",
      "Original: aAFF BSP aOTR -> Broader: Academic_Feedback Praise Opportunity_to_Respond\n",
      "Original: SV IST BSP aOTR SV -> Broader: Other Praise Opportunity_to_Respond Teacher_Talk\n",
      "Original: aOTR -> Broader: Opportunity_to_Respond\n"
     ]
    }
   ],
   "source": [
    "# Create mapping dictionary from specific labels to broader concepts\n",
    "# Using underscores to keep multi-word concepts together\n",
    "label_to_concept = {\n",
    "    'IST': 'Teacher_Talk',\n",
    "    'ST': 'Teacher_Talk',\n",
    "    'aOTR': 'Opportunity_to_Respond',\n",
    "    'sOTR': 'Opportunity_to_Respond',\n",
    "    'REP': 'Corrective_Behavioral_Feedback',\n",
    "    'RED': 'Corrective_Behavioral_Feedback',\n",
    "    'GPRS': 'Praise',\n",
    "    'BSP': 'Praise',\n",
    "    'aAFF': 'Academic_Feedback',\n",
    "    'aCORR': 'Academic_Feedback',\n",
    "    'SV': 'Other',\n",
    "    'NEU': 'Other'\n",
    "}\n",
    "\n",
    "# Function to map detailed labels to broader concepts\n",
    "def map_to_broader_concepts(label_string):\n",
    "    specific_labels = label_string.split()\n",
    "    broader_concepts = set(label_to_concept[label] for label in specific_labels)  # using set to remove duplicates\n",
    "    return ' '.join(broader_concepts)\n",
    "\n",
    "# Transform labels in all datasets\n",
    "train_df['broader_label'] = train_df['label'].apply(map_to_broader_concepts)\n",
    "val_df['broader_label'] = val_df['label'].apply(map_to_broader_concepts)\n",
    "test_df['broader_label'] = test_df['label'].apply(map_to_broader_concepts)\n",
    "\n",
    "# Create new MultiLabelBinarizer for broader concepts\n",
    "broader_mlb = MultiLabelBinarizer()\n",
    "train_broader_labels = train_df['broader_label'].str.split()\n",
    "broader_mlb.fit(train_broader_labels)\n",
    "\n",
    "# Transform all datasets with broader concepts\n",
    "train_encoded_labels = broader_mlb.transform(train_broader_labels)\n",
    "val_encoded_labels = broader_mlb.transform(val_df['broader_label'].str.split())\n",
    "test_encoded_labels = broader_mlb.transform(test_df['broader_label'].str.split())\n",
    "\n",
    "# Let's see the distribution of broader concepts\n",
    "print(\"Broader concept frequencies in training set:\")\n",
    "label_frequencies = train_encoded_labels.sum(axis=0)\n",
    "label_stats = pd.DataFrame({\n",
    "    'Broader Concept': broader_mlb.classes_,\n",
    "    'Count': label_frequencies,\n",
    "    'Percentage': (label_frequencies / len(train_encoded_labels)) * 100\n",
    "}).sort_values('Count', ascending=False)\n",
    "print(label_stats.to_string(index=False))\n",
    "\n",
    "print(\"\\nDistribution of number of broader concepts per instance:\")\n",
    "label_counts_per_instance = train_encoded_labels.sum(axis=1)\n",
    "unique_counts, counts = np.unique(label_counts_per_instance, return_counts=True)\n",
    "for num_labels, count in zip(unique_counts, counts):\n",
    "    percentage = (count / len(train_encoded_labels)) * 100\n",
    "    print(f\"{num_labels} concepts: {count} instances ({percentage:.2f}%)\")\n",
    "\n",
    "# Let's also check a few examples to make sure it's working correctly\n",
    "print(\"\\nSample mappings:\")\n",
    "for _, row in train_df[['label', 'broader_label']].head(10).iterrows():\n",
    "    print(f\"Original: {row['label']} -> Broader: {row['broader_label']}\")# Create datasets with broader concepts\n",
    "train_dataset = TextClassificationDataset(\n",
    "    train_df['transcript'].values, \n",
    "    train_encoded_labels,\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "val_dataset = TextClassificationDataset(\n",
    "    val_df['transcript'].values, \n",
    "    val_encoded_labels,\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "test_dataset = TextClassificationDataset(\n",
    "    test_df['transcript'].values, \n",
    "    test_encoded_labels,\n",
    "    tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7c7ce3-3b95-4c7d-b2a5-f61bf8ac9b45",
   "metadata": {},
   "source": [
    "## Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "492aab7b-5058-41f7-905a-ce7c128f80c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Create data loaders with original batch size\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Initialize model with gradient checkpointing for memory efficiency\n",
    "model = RobertaForSequenceClassification.from_pretrained(\n",
    "    'roberta-base',\n",
    "    num_labels=len(broader_mlb.classes_),\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")\n",
    "model.gradient_checkpointing_enable()  # Enable gradient checkpointing to save memory\n",
    "\n",
    "# Training setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3780e555-5d22-4503-890b-c710f668f413",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be93aa27-ce96-4894-88b0-727731da8d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping parameters\n",
    "patience = 3\n",
    "min_delta = 0.001\n",
    "patience_counter = 0\n",
    "early_stop = False\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "best_val_loss = float('inf')\n",
    "best_macro_f1 = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "385ba9dc-ad06-44bb-af40-863c81c3d680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Training Loss: 0.4437\n",
      "Validation Loss: 0.4140\n",
      "Macro F1: 0.5744\n",
      "Micro F1: 0.7665\n",
      "\n",
      "Per-concept metrics:\n",
      "Academic_Feedback         - F1: 0.6977, Precision: 0.8824, Recall: 0.5769\n",
      "Corrective_Behavioral_Feedback - F1: 0.0000, Precision: 0.0000, Recall: 0.0000\n",
      "Opportunity_to_Respond    - F1: 0.8871, Precision: 0.8652, Recall: 0.9102\n",
      "Other                     - F1: 0.7529, Precision: 0.6037, Recall: 1.0000\n",
      "Praise                    - F1: 0.3500, Precision: 0.3443, Recall: 0.3559\n",
      "Teacher_Talk              - F1: 0.7589, Precision: 0.6825, Recall: 0.8546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/miniconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best model based on Macro F1!\n",
      "Metrics improved!\n",
      "\n",
      "Epoch 2\n",
      "Training Loss: 0.3794\n",
      "Validation Loss: 0.3839\n",
      "Macro F1: 0.6357\n",
      "Micro F1: 0.7866\n",
      "\n",
      "Per-concept metrics:\n",
      "Academic_Feedback         - F1: 0.7891, Precision: 0.8406, Recall: 0.7436\n",
      "Corrective_Behavioral_Feedback - F1: 0.1538, Precision: 1.0000, Recall: 0.0833\n",
      "Opportunity_to_Respond    - F1: 0.8878, Precision: 0.9169, Recall: 0.8605\n",
      "Other                     - F1: 0.7529, Precision: 0.6037, Recall: 1.0000\n",
      "Praise                    - F1: 0.4557, Precision: 0.9000, Recall: 0.3051\n",
      "Teacher_Talk              - F1: 0.7745, Precision: 0.6786, Recall: 0.9021\n",
      "Saved best model based on Macro F1!\n",
      "Metrics improved!\n",
      "\n",
      "Epoch 3\n",
      "Training Loss: 0.3388\n",
      "Validation Loss: 0.3685\n",
      "Macro F1: 0.6822\n",
      "Micro F1: 0.7949\n",
      "\n",
      "Per-concept metrics:\n",
      "Academic_Feedback         - F1: 0.8162, Precision: 0.7939, Recall: 0.8397\n",
      "Corrective_Behavioral_Feedback - F1: 0.3529, Precision: 0.6000, Recall: 0.2500\n",
      "Opportunity_to_Respond    - F1: 0.8940, Precision: 0.9221, Recall: 0.8676\n",
      "Other                     - F1: 0.7372, Precision: 0.6096, Recall: 0.9324\n",
      "Praise                    - F1: 0.4938, Precision: 0.9091, Recall: 0.3390\n",
      "Teacher_Talk              - F1: 0.7989, Precision: 0.7170, Recall: 0.9021\n",
      "Saved best model based on Macro F1!\n",
      "Metrics improved!\n",
      "\n",
      "Epoch 4\n",
      "Training Loss: 0.3098\n",
      "Validation Loss: 0.3707\n",
      "Macro F1: 0.7148\n",
      "Micro F1: 0.7992\n",
      "\n",
      "Per-concept metrics:\n",
      "Academic_Feedback         - F1: 0.8014, Precision: 0.8779, Recall: 0.7372\n",
      "Corrective_Behavioral_Feedback - F1: 0.4615, Precision: 0.6000, Recall: 0.3750\n",
      "Opportunity_to_Respond    - F1: 0.8961, Precision: 0.9160, Recall: 0.8771\n",
      "Other                     - F1: 0.7489, Precision: 0.6017, Recall: 0.9915\n",
      "Praise                    - F1: 0.5800, Precision: 0.7073, Recall: 0.4915\n",
      "Teacher_Talk              - F1: 0.8011, Precision: 0.7282, Recall: 0.8902\n",
      "Saved best model based on Macro F1!\n",
      "Metrics improved!\n",
      "\n",
      "Epoch 5\n",
      "Training Loss: 0.2713\n",
      "Validation Loss: 0.3900\n",
      "Macro F1: 0.7056\n",
      "Micro F1: 0.7874\n",
      "\n",
      "Per-concept metrics:\n",
      "Academic_Feedback         - F1: 0.8027, Precision: 0.8392, Recall: 0.7692\n",
      "Corrective_Behavioral_Feedback - F1: 0.4681, Precision: 0.4783, Recall: 0.4583\n",
      "Opportunity_to_Respond    - F1: 0.8850, Precision: 0.9275, Recall: 0.8463\n",
      "Other                     - F1: 0.7387, Precision: 0.6069, Recall: 0.9437\n",
      "Praise                    - F1: 0.5470, Precision: 0.5517, Recall: 0.5424\n",
      "Teacher_Talk              - F1: 0.7923, Precision: 0.7009, Recall: 0.9110\n",
      "No improvement in either metric. Patience: 1/3\n",
      "\n",
      "Training Summary:\n",
      "Best validation loss: 0.3685\n",
      "Best macro F1: 0.7148\n",
      "Training stopped after 5 epochs\n",
      "Reason: Completed all epochs\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Clear memory\n",
    "        del outputs, loss\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_preds = []\n",
    "    val_true = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            preds = torch.sigmoid(outputs.logits).cpu().numpy()\n",
    "            val_preds.extend(preds)\n",
    "            val_true.extend(labels.cpu().numpy())\n",
    "            \n",
    "            # Clear memory\n",
    "            del outputs, loss\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    gc.collect()  # Garbage collection\n",
    "    \n",
    "    # Calculate metrics\n",
    "    val_preds = np.array(val_preds) > 0.5\n",
    "    val_true = np.array(val_true)\n",
    "    \n",
    "    avg_val_loss = val_loss/len(val_loader)\n",
    "    macro_f1 = f1_score(val_true, val_preds, average='macro')\n",
    "    micro_f1 = f1_score(val_true, val_preds, average='micro')\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch + 1}\")\n",
    "    print(f\"Training Loss: {total_loss/len(train_loader):.4f}\")\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "    print(f\"Micro F1: {micro_f1:.4f}\")\n",
    "    \n",
    "    # Print metrics for each broader concept\n",
    "    print(\"\\nPer-concept metrics:\")\n",
    "    for i, concept in enumerate(broader_mlb.classes_):\n",
    "        concept_f1 = f1_score(val_true[:, i], val_preds[:, i])\n",
    "        precision = precision_score(val_true[:, i], val_preds[:, i])\n",
    "        recall = recall_score(val_true[:, i], val_preds[:, i])\n",
    "        print(f\"{concept:25} - F1: {concept_f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n",
    "\n",
    "    # Save best model and check early stopping based on both metrics\n",
    "    improved = False\n",
    "    \n",
    "    if macro_f1 > best_macro_f1 + min_delta:\n",
    "        best_macro_f1 = macro_f1\n",
    "        improved = True\n",
    "        torch.save(model.state_dict(), 'best_model_whisper_diarization_broader_concepts.pt')\n",
    "        print(\"Saved best model based on Macro F1!\")\n",
    "    \n",
    "    if avg_val_loss < best_val_loss - min_delta:\n",
    "        best_val_loss = avg_val_loss\n",
    "        improved = True\n",
    "    \n",
    "    # Update early stopping counter\n",
    "    if not improved:\n",
    "        patience_counter += 1\n",
    "        print(f\"No improvement in either metric. Patience: {patience_counter}/{patience}\")\n",
    "    else:\n",
    "        patience_counter = 0\n",
    "        print(\"Metrics improved!\")\n",
    "\n",
    "    # Check early stopping condition\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping triggered after {epoch + 1} epochs!\")\n",
    "        early_stop = True\n",
    "        break\n",
    "\n",
    "    # Clear memory after each epoch\n",
    "    del val_preds, val_true\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Print training summary\n",
    "print(\"\\nTraining Summary:\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"Best macro F1: {best_macro_f1:.4f}\")\n",
    "print(f\"Training stopped after {epoch + 1} epochs\")\n",
    "if early_stop:\n",
    "    print(\"Reason: Early stopping\")\n",
    "else:\n",
    "    print(\"Reason: Completed all epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb33fe99-1fba-4ff2-bc82-273184fba46b",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "177fb02e-3bbd-4741-a2b9-3d2f7cff1042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test Set Results:\n",
      "Macro F1: 0.6619\n",
      "Micro F1: 0.7754\n",
      "\n",
      "Per-concept test metrics:\n",
      "Academic_Feedback         - F1: 0.5965, Precision: 0.6892, Recall: 0.5258\n",
      "Corrective_Behavioral_Feedback - F1: 0.3844, Precision: 0.5074, Recall: 0.3094\n",
      "Opportunity_to_Respond    - F1: 0.7894, Precision: 0.8205, Recall: 0.7606\n",
      "Other                     - F1: 0.8119, Precision: 0.6871, Recall: 0.9919\n",
      "Praise                    - F1: 0.5787, Precision: 0.6441, Recall: 0.5253\n",
      "Teacher_Talk              - F1: 0.8105, Precision: 0.7385, Recall: 0.8980\n"
     ]
    }
   ],
   "source": [
    "# Load best model and evaluate on test set\n",
    "model.load_state_dict(torch.load('best_model_whisper_diarization_broader_concepts.pt'))\n",
    "model.eval()\n",
    "test_preds = []\n",
    "test_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        preds = torch.sigmoid(outputs.logits).cpu().numpy()\n",
    "        test_preds.extend(preds)\n",
    "        test_true.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Clear memory\n",
    "        del outputs\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "test_preds = np.array(test_preds) > 0.5\n",
    "test_true = np.array(test_true)\n",
    "\n",
    "print(\"\\nFinal Test Set Results:\")\n",
    "print(f\"Macro F1: {f1_score(test_true, test_preds, average='macro'):.4f}\")\n",
    "print(f\"Micro F1: {f1_score(test_true, test_preds, average='micro'):.4f}\")\n",
    "\n",
    "print(\"\\nPer-concept test metrics:\")\n",
    "for i, concept in enumerate(broader_mlb.classes_):\n",
    "    concept_f1 = f1_score(test_true[:, i], test_preds[:, i])\n",
    "    precision = precision_score(test_true[:, i], test_preds[:, i])\n",
    "    recall = recall_score(test_true[:, i], test_preds[:, i])\n",
    "    print(f\"{concept:25} - F1: {concept_f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "50af28e9-3df8-46e0-a02d-6f722525234e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3b7bb1e8-4b13-4f93-b0ff-41788092234c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.53      0.60       426\n",
      "           1       0.51      0.31      0.38       223\n",
      "           2       0.82      0.76      0.79      1412\n",
      "           3       0.69      0.99      0.81      1975\n",
      "           4       0.64      0.53      0.58       217\n",
      "           5       0.74      0.90      0.81      1774\n",
      "\n",
      "   micro avg       0.72      0.84      0.78      6027\n",
      "   macro avg       0.68      0.67      0.66      6027\n",
      "weighted avg       0.73      0.84      0.77      6027\n",
      " samples avg       0.72      0.84      0.75      6027\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_true, test_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
